# PJB: Use the VENV in Development/HayStack
#
# -----------------------------------------------------------------------------
# Reads the output from lucris-rs (-r parameter) and creates a document
# store (-d parameter).
# 
# We could extract names (and other meta data) from the query
# using a LLM and use it in the retrieval.
# Uses researcher name, title and abstract (as generated by lucris-rs).
# -----------------------------------------------------------------------------
#
from haystack import Pipeline
from haystack import Document
from haystack.document_stores.in_memory import InMemoryDocumentStore
from haystack.components.retrievers.in_memory import InMemoryBM25Retriever
from haystack.components.converters import TextFileToDocument
from haystack.components.preprocessors import DocumentCleaner
from haystack.components.preprocessors import DocumentSplitter
from haystack.document_stores.types import DuplicatePolicy
from haystack.components.writers import DocumentWriter
from haystack.components.rankers import LostInTheMiddleRanker
from haystack.components.rankers import TransformersSimilarityRanker
from haystack.components.rankers import SentenceTransformersDiversityRanker
from haystack_integrations.components.generators.ollama import OllamaGenerator
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack.components.builders import PromptBuilder
from haystack.components.joiners import DocumentJoiner
import argparse
import logging

# Create a logger
logger = logging.getLogger('foo')
logger.setLevel(logging.DEBUG)

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler = logging.FileHandler('haystack.log')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s', "%Y-%m-%d %H:%M:%S")
console_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)

parser = argparse.ArgumentParser()
parser.add_argument("-r", "--research", help="Research file.", default=None)
parser.add_argument("-s", "--storename", help="Document store.", default="docs_research.store")
parser.add_argument("-t", "--test", action='store_true', help="Run a small test.", default=False)
args = parser.parse_args()

logger.debug(args)

# -----------------------------------------------------------------------------

#RESEARCH: a06df509-b7e0-474a-b84a-3376a72f9e56
#PERSON0: Karin Johansson e1b388c9-685a-41d6-84cc-b217e14bbff3
#PERSON1: Nisse Johansson e1b388c9-685a-41d6-84cc-b217e14bbff4
#TITLE: A randomized study ...
#ABSTRACT: We compared manual ...
#RESEARCH: 740c676d-7ab4-4975-a1c6-d4d0d2976092

# We could have defaults for the other values as well.
def get_new_meta() -> dict:
    # return {"persons":[]} # initialise empty list
    return {}

# name, title abstract format.
# Reads the file created by lucris-rs and returns a list
# with Documents.
def read_research_nta(a_file) -> [Document]:
    current_content = None
    current_meta = get_new_meta()
    documents = []
    with open(a_file, "r") as f:
        for line in f:
            line = line.strip()
            if line.startswith("NAME"):  # Matches NAME: and NAMES:
                bits = line.split(":", 1)
                if len(bits) > 0:
                    name = bits[1]
                    #print("NAME", name)
                    # If we have current contents, we save it first.
                    if current_content and current_meta:
                        doc = Document(content=current_content, meta=current_meta)
                        documents.append(doc)
                        print("ADDED", current_meta)
                    current_meta = get_new_meta()
                    current_meta["researcher_name"] = name.strip()
                    current_content = None
            elif line.startswith("TITLE:"):
                bits = line.split(":", 1)
                if len(bits) > 0:
                    title = bits[1]
                    #print("TITLE", title)
                    current_meta["title"] = title.strip()
            elif line.startswith("ABSTRACT:"):
                bits = line.split(":", 1)
                if len(bits) > 0:
                    # abstract can be empty... mirror title?
                    abstract = bits[1].strip()
                    #print(current_meta)
                    if len(abstract) < 2: #some arbitrary small value
                        try:
                            abstract = current_meta["title"] # We assume we have read it... FIXME
                        except KeyError:
                            abstract = "no abstract"
                    current_content = abstract
                    current_meta["abstract"] = abstract
    # Left overs.
    if current_content and current_meta:
        doc = Document(content=current_content, meta=current_meta)
        documents.append(doc)
        print("ADDED", current_meta)
    return documents

# -----------------------------------------------------------------------------

#if args.research:
def embed_and_store(filename, storename):
    docs = read_research_nta(filename)
    print("Doc count:", len(docs))
    print(docs[0])

    # We only have abstracts, splitting doesn't really make sense.
    document_splitter = DocumentSplitter(
        split_by="word", 
        split_length=512, 
        split_overlap=64
    )
    document_embedder = SentenceTransformersDocumentEmbedder(
        model="sentence-transformers/all-MiniLM-L6-v2",
        meta_fields_to_embed=["title", "researcher_name"]
    )
    document_store = InMemoryDocumentStore()
    document_writer = DocumentWriter(
        document_store=document_store,
        policy=DuplicatePolicy.SKIP
    )
    
    indexing_pipeline = Pipeline()
    indexing_pipeline.add_component("document_splitter", document_splitter)
    indexing_pipeline.add_component("document_embedder", document_embedder)
    indexing_pipeline.add_component("document_writer", document_writer)
    
    # Also write for BM25 index?

    indexing_pipeline.connect("document_splitter", "document_embedder")
    indexing_pipeline.connect("document_embedder", "document_writer")
    indexing_pipeline.run(
        {
            "document_splitter": {"documents": docs}
        }
    )
    logger.info(f"Writing {storename}...");
    document_store.save_to_disk(storename)

def run_test(storename):
    logger.info("Loading document store...")
    document_store_new = InMemoryDocumentStore().load_from_disk(storename)
    logger.info(f"Number of documents: {document_store_new.count_documents()}.")

    query = "What are heavy resonances?"
    retrieve_top_k = 3

    # Test 1
    retriever = InMemoryEmbeddingRetriever(document_store_new)
    #doc_embedder = SentenceTransformersDocumentEmbedder(
    #    model="sentence-transformers/all-MiniLM-L6-v2", # Dim depends on model.
    #    meta_fields_to_embed=["title", "researcher_name"]
    #)
    #doc_embedder.warm_up()
    text_embedder = SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2")
    #text_embedder = SentenceTransformersTextEmbedder()
    query_pipeline = Pipeline() 
    query_pipeline.add_component("text_embedder", text_embedder)
    result = query_pipeline.run({"text_embedder": {"text": query}})
    q_embedding = result['text_embedder']['embedding']
    
    res = retriever.run(
        query_embedding=q_embedding,
        top_k=retrieve_top_k,
        scale_score=True
    )
    logger.info("EMBEDDINGS: Retrieved documents")
    for i, r in enumerate(res["documents"]): # add ["document_joiner"] if experimental
        #logger.info(r)
        logger.info(f"{i:02n} {r.score:.4f} {r.meta['researcher_name']}")
        logger.info(f"    {r.content}")
        logger.info("-" * 78)
    logger.info("")
    logger.info("=" * 78)

    logger.info("")

    #test 2
    retriever = InMemoryBM25Retriever(document_store=document_store_new)
    res = retriever.run(
        query=query,
        top_k=retrieve_top_k,
        scale_score=True
    )
    logger.info("BM25: Retrieved documents")
    for i, r in enumerate(res["documents"]): # add ["document_joiner"] if experimental
        #logger.info(r)
        logger.info(f"{i:02n} {r.score:.4f} {r.meta['researcher_name']}")
        logger.info(f"    {r.content}")
        logger.info("-" * 78)
    logger.info("")
    logger.info("=" * 78)

# -----------------------------------------------------------------------------

if __name__ == '__main__':
    if args.research and args.storename:
        embed_and_store(args.research, args.storename)
    if  args.test and args.storename:
        run_test(args.storename)
    
